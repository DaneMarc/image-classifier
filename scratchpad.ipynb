{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d017333",
   "metadata": {},
   "source": [
    "# Scratch Pad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022cb4cd",
   "metadata": {},
   "source": [
    "## Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c14a2d8",
   "metadata": {},
   "source": [
    "##### Overview\n",
    "Before settling on my final model, I mainly explored using a convolution neural network (CNN) and a normal linear neural network. I thought of using a CNN as judging from the given data, we had to classify images and CNNs were supposedly better for that kind of data. Unfortunately, I could not tune my CNN well enough and got consistently low F1 scores. I then settled on using a linear model which was quicker and while inconsistent, still gave higher F1 scores overall. I mostly spent my time after that tuning the number of layers and the hyperparameters to make it more consistent and accurate.\n",
    "\n",
    "##### 1. Descriptive Analysis\n",
    "As suggested, I found the mean, median, mode, standard deviation and range of the data. Judging from the min and max values as well as the standard deviation, I realised there were quite a number of outliers that needed to be handled. The mode also showed a lot of 0s and 255s so i just assumed the intended range of values was 0 - 255.\n",
    "\n",
    "##### 2. Detection and Handling of Missing Values\n",
    "There were NaN values in the data which I replaced with the channel mean of the pixel's or value's respective channel in that image. I thought this was a good compromise as this would not skew the data while being easy to compute. I thought of using a more local mean for a more accurate representation by getting the mean of the values directly around the NaN value but decided against it due to efficiency reasons.\n",
    "\n",
    "##### 3. Detection and Handling of Outliers\n",
    "I found out that the outliers (values below 0 and above 255) made up around 2% of the data which was not insignificant. Thus, I replaced them with the channel mean the same as how the NaN values were handled for the same reason.\n",
    "\n",
    "##### 4. Detection and Handling of Class Imbalance \n",
    "The classes were very imbalanced (2392, 203, 25) which made me decide to use cost-sensitive training as I felt that undersampling or oversampling would lose too much data or not provide a good representation of the data.\n",
    "\n",
    "##### 5. Understanding Relationship Between Variables\n",
    "I skipped this part. Considering the number of variables (768 if image flattened), I decided it was best to let PCA reduce dimensionality instead of manually doing it myself.\n",
    "\n",
    "##### 6. Data Visualization\n",
    "I also skipped this part for the same reason above.\n",
    "\n",
    "##### 7. General Preprocessing\n",
    "- I preprocessed the data by first replacing the outliers with NaN values since I was going to be treating both the same by replacing them with their channel mean.\n",
    "- I went through all the data replacing NaN values with their channel mean.\n",
    "- I flattened the images and standardised them by using sklearn's StandardScaler to prepare them for PCA. I used MinMaxScaler at first but found out that since I technically already normalised the range by eliminating outliers, the StandardScaler should technically work better. In practice there was minimal improvement if at all.\n",
    " \n",
    "##### 8. Feature Selection \n",
    "I used sklearn's PCA to reduce dimensionality and thus noise. For the number of components I chose 0.99 which will result in a calculated number of components to retain until they collectively explain at least 99% of the variance in the original data. This was a conservative option as it reduced the number of components to around 600 most of the time. I experimented using 0.95 or 95% variance but there was noticeable deprovements in accuracy so I stuck with 0.99.\n",
    "\n",
    "##### 9. Feature Engineering\n",
    "I skipped this part as I wanted to keep it simple at first and ended up having no time to try this.\n",
    "\n",
    "##### 10. Creating Models\n",
    "I initially used 1 hidden linear layer with 256 or 128 parameters which on average gave me around a 0.5 F1 score. I also used CrossEntropyLoss as it allowed inputting custom weights to handle the class imbalance. I also used sklearn's class_weight compute to help me compute a balanced distribution which I could pass to the loss function to use. For the optimiser, I settled on Adam as it usually provided the best accuracy as opposed to Adagrad or SGD. Learning rate is 1e-3 which is standard but also worked well with the smaller batch numbers that I found provided more consistent results. I toyed with batch numbers up to 512 but settled with 64 and 128 as they proved more consistent. I also added more hidden layers and settled on 3 as I found it provided consistently better accuracy without too much of a hit on performance. The parameters I used were 1024, 512 and 256.\n",
    "\n",
    "##### 11. Model Evaluation\n",
    "I mostly just went by loss and testing using the F1 score provided in the main jupyter notebook as I did not have much time to set up my own testing suite.\n",
    "\n",
    "##### 12. Hyperparameters Search\n",
    "I skipped this as I had no time so I just randomly changed things that somewhat made sense like learning rate and batch size.\n",
    "\n",
    "##### Conclusion\n",
    "To conclude, while my model is not conventionally used for image classification, it's performance was satisfactory and it was easily tunable and generalisable which is good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dcaf29",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27103374",
   "metadata": {},
   "source": [
    "# Workings (Not Graded)\n",
    "\n",
    "You will do your working below. Note that anything below this section will not be graded, but we might counter-check what you wrote in the report above with your workings to make sure that you actually did what you claimed to have done. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4c6cd4",
   "metadata": {},
   "source": [
    "## Import Packages\n",
    "\n",
    "Here, we import some packages necessary to run this notebook. In addition, you may import other packages as well. Do note that when submitting your model, you may only use packages that are available in Coursemology (see `main.ipynb`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cded1ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748c35d7",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "The dataset `data/images.npy` is of size $(N, C, H, W)$, where $N$, $C$, $H$, and $W$ correspond to the number of data, image channels, image width, and image height, respectively.\n",
    "\n",
    "A code snippet that loads the data is provided below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09da291",
   "metadata": {},
   "source": [
    "### Load Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6297e25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (2911, 3, 16, 16)\n"
     ]
    }
   ],
   "source": [
    "with open('data.npy', 'rb') as f:\n",
    "    data = np.load(f, allow_pickle=True).item()\n",
    "    images = data['image']\n",
    "    labels = data['label']\n",
    "    \n",
    "print('Shape:', images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float16\n",
      "(array([ 0.,  1.,  2., nan], dtype=float16), array([2392,  203,   25,  291]))\n",
      "-10000.0 10000.0\n",
      "0.98016324573457 %\n"
     ]
    }
   ],
   "source": [
    "print(images.dtype)\n",
    "print(np.unique(labels, return_counts=True))\n",
    "\n",
    "print(np.nanmin(images), np.nanmax(images))\n",
    "\n",
    "#print(images[1])\n",
    "\n",
    "nonnan = np.count_nonzero(~np.isnan(images))\n",
    "nan = np.count_nonzero(np.isnan(images))\n",
    "print(nan / (nonnan + nan) * 100, '%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe832b6",
   "metadata": {},
   "source": [
    "## Data Exploration & Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6a464c",
   "metadata": {},
   "source": [
    "### 1. Descriptive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3b1f62dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive Analysis:\n",
      "Mean: 91.95161440252559\n",
      "Median: 78.0\n",
      "Range: 20000.0\n",
      "Standard Deviation: 823.0422541579951\n",
      "Variance: 677398.5521294738\n",
      "Interquartile Range: 93.875\n"
     ]
    }
   ],
   "source": [
    "# Descriptive Analysis\n",
    "data_summary = {\n",
    "    'Mean': np.nanmean(images.astype('float64'), dtype=np.float64),\n",
    "    'Median': np.nanmedian(images),\n",
    "    'Mode': stats.mode(images, nan_policy='omit').mode[0],\n",
    "    'Range': np.nanmax(images) - np.nanmin(images),\n",
    "    'Standard Deviation': np.nanstd(images.astype('float64'), dtype=np.float64),\n",
    "    'Variance': np.nanvar(images.astype('float64'), dtype=np.float64),\n",
    "    'Interquartile Range': np.nanpercentile(images, 75) - np.nanpercentile(images, 25)\n",
    "}\n",
    "\n",
    "print(\"Descriptive Analysis:\")\n",
    "for key, value in data_summary.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb61967",
   "metadata": {},
   "source": [
    "### 2. Detection and Handling of Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4bb9cdfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21913\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(np.count_nonzero(np.isnan(images)))\n",
    "\n",
    "for i in range(images.shape[0]):\n",
    "    # Extract the current image\n",
    "    image = images[i, :, :, :]\n",
    "\n",
    "    # Check if there are any NaN values in the image\n",
    "    if np.isnan(image).any():\n",
    "        # Calculate the mean value of each channel\n",
    "        channel_means = np.nanmean(image.astype('float64'), axis=(1, 2))\n",
    "        if np.isnan(channel_means).any():\n",
    "            print(\"There are NaN values in the channel means\")\n",
    "\n",
    "        # Replace NaN values with the corresponding channel mean\n",
    "        image[0][np.isnan(image[0])] = channel_means[0]\n",
    "        image[1][np.isnan(image[1])] = channel_means[1]\n",
    "        image[2][np.isnan(image[2])] = channel_means[2]\n",
    "\n",
    "    # Put the resolved image back into the image array\n",
    "    images[i, :, :, :] = image\n",
    "\n",
    "print(np.count_nonzero(np.isnan(images)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adcb9cd",
   "metadata": {},
   "source": [
    "### 3. Detection and Handling of Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ed1c17a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9826681123325317 %\n",
      "0.9975631226382685 %\n",
      "ModeResult(mode=8880.0, count=32)\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "above = (images > 255).sum()\n",
    "print(above / (images.size) * 100, '%')\n",
    "below = (images < 0).sum()\n",
    "print(below / (images.size) * 100, '%')\n",
    "\n",
    "print(stats.mode(images[images > 255], nan_policy='omit'))\n",
    "\n",
    "np.clip(images, 0, 255, out=images)\n",
    "print((images > 255).sum())\n",
    "print((images < 0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f20e969d9c0>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[2200].astype('int').T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4916043",
   "metadata": {},
   "source": [
    "### 4. Detection and Handling of Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ad3ab20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.,  1.,  2., nan], dtype=float16), array([2392,  203,   25,  291]))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(labels, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "unq, count = np.unique(images, axis=0, return_counts=True)\n",
    "print(images[count > 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7eebcf",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4921e8ca",
   "metadata": {},
   "source": [
    "### 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "dbcde626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.003041129403455104\n",
      "645\n",
      "768\n",
      "2911\n"
     ]
    }
   ],
   "source": [
    "print(np.count_nonzero(np.isnan(images)))\n",
    "reshaped = images.reshape(images.shape[0], -1)\n",
    "scaler = StandardScaler()\n",
    "reshaped_scaled = scaler.fit_transform(reshaped)\n",
    "\n",
    "pca = PCA(n_components=0.99)\n",
    "pca.fit(reshaped_scaled)\n",
    "print(pca.noise_variance_)\n",
    "print(pca.n_components_)\n",
    "print(pca.n_features_)\n",
    "print(pca.n_samples_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_scaled = pca.transform(reshaped_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa676c3f",
   "metadata": {},
   "source": [
    "## Modeling & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589b37e4",
   "metadata": {},
   "source": [
    "### 6. Creating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove nan values from labels and data\n",
    "reshaped_scaled = reshaped_scaled[~np.isnan(labels)]\n",
    "labels = labels[~np.isnan(labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2.]\n",
      "(2620, 645)\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(labels))\n",
    "print(reshaped_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d8dffd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 1.1078\n",
      "Epoch [2/100], Loss: 1.0667\n",
      "Epoch [3/100], Loss: 1.0301\n",
      "Epoch [4/100], Loss: 0.9965\n",
      "Epoch [5/100], Loss: 0.9650\n",
      "Epoch [6/100], Loss: 0.9350\n",
      "Epoch [7/100], Loss: 0.9060\n",
      "Epoch [8/100], Loss: 0.8773\n",
      "Epoch [9/100], Loss: 0.8488\n",
      "Epoch [10/100], Loss: 0.8206\n",
      "Epoch [11/100], Loss: 0.7926\n",
      "Epoch [12/100], Loss: 0.7650\n",
      "Epoch [13/100], Loss: 0.7379\n",
      "Epoch [14/100], Loss: 0.7115\n",
      "Epoch [15/100], Loss: 0.6859\n",
      "Epoch [16/100], Loss: 0.6611\n",
      "Epoch [17/100], Loss: 0.6374\n",
      "Epoch [18/100], Loss: 0.6146\n",
      "Epoch [19/100], Loss: 0.5927\n",
      "Epoch [20/100], Loss: 0.5718\n",
      "Epoch [21/100], Loss: 0.5517\n",
      "Epoch [22/100], Loss: 0.5324\n",
      "Epoch [23/100], Loss: 0.5138\n",
      "Epoch [24/100], Loss: 0.4958\n",
      "Epoch [25/100], Loss: 0.4784\n",
      "Epoch [26/100], Loss: 0.4616\n",
      "Epoch [27/100], Loss: 0.4452\n",
      "Epoch [28/100], Loss: 0.4294\n",
      "Epoch [29/100], Loss: 0.4141\n",
      "Epoch [30/100], Loss: 0.3993\n",
      "Epoch [31/100], Loss: 0.3850\n",
      "Epoch [32/100], Loss: 0.3712\n",
      "Epoch [33/100], Loss: 0.3578\n",
      "Epoch [34/100], Loss: 0.3448\n",
      "Epoch [35/100], Loss: 0.3323\n",
      "Epoch [36/100], Loss: 0.3201\n",
      "Epoch [37/100], Loss: 0.3084\n",
      "Epoch [38/100], Loss: 0.2970\n",
      "Epoch [39/100], Loss: 0.2859\n",
      "Epoch [40/100], Loss: 0.2753\n",
      "Epoch [41/100], Loss: 0.2649\n",
      "Epoch [42/100], Loss: 0.2549\n",
      "Epoch [43/100], Loss: 0.2453\n",
      "Epoch [44/100], Loss: 0.2359\n",
      "Epoch [45/100], Loss: 0.2269\n",
      "Epoch [46/100], Loss: 0.2181\n",
      "Epoch [47/100], Loss: 0.2097\n",
      "Epoch [48/100], Loss: 0.2016\n",
      "Epoch [49/100], Loss: 0.1938\n",
      "Epoch [50/100], Loss: 0.1862\n",
      "Epoch [51/100], Loss: 0.1790\n",
      "Epoch [52/100], Loss: 0.1720\n",
      "Epoch [53/100], Loss: 0.1653\n",
      "Epoch [54/100], Loss: 0.1588\n",
      "Epoch [55/100], Loss: 0.1526\n",
      "Epoch [56/100], Loss: 0.1466\n",
      "Epoch [57/100], Loss: 0.1408\n",
      "Epoch [58/100], Loss: 0.1353\n",
      "Epoch [59/100], Loss: 0.1300\n",
      "Epoch [60/100], Loss: 0.1248\n",
      "Epoch [61/100], Loss: 0.1199\n",
      "Epoch [62/100], Loss: 0.1152\n",
      "Epoch [63/100], Loss: 0.1107\n",
      "Epoch [64/100], Loss: 0.1064\n",
      "Epoch [65/100], Loss: 0.1022\n",
      "Epoch [66/100], Loss: 0.0983\n",
      "Epoch [67/100], Loss: 0.0945\n",
      "Epoch [68/100], Loss: 0.0908\n",
      "Epoch [69/100], Loss: 0.0873\n",
      "Epoch [70/100], Loss: 0.0840\n",
      "Epoch [71/100], Loss: 0.0808\n",
      "Epoch [72/100], Loss: 0.0778\n",
      "Epoch [73/100], Loss: 0.0749\n",
      "Epoch [74/100], Loss: 0.0721\n",
      "Epoch [75/100], Loss: 0.0695\n",
      "Epoch [76/100], Loss: 0.0670\n",
      "Epoch [77/100], Loss: 0.0645\n",
      "Epoch [78/100], Loss: 0.0622\n",
      "Epoch [79/100], Loss: 0.0600\n",
      "Epoch [80/100], Loss: 0.0579\n",
      "Epoch [81/100], Loss: 0.0559\n",
      "Epoch [82/100], Loss: 0.0540\n",
      "Epoch [83/100], Loss: 0.0521\n",
      "Epoch [84/100], Loss: 0.0504\n",
      "Epoch [85/100], Loss: 0.0487\n",
      "Epoch [86/100], Loss: 0.0471\n",
      "Epoch [87/100], Loss: 0.0456\n",
      "Epoch [88/100], Loss: 0.0441\n",
      "Epoch [89/100], Loss: 0.0427\n",
      "Epoch [90/100], Loss: 0.0413\n",
      "Epoch [91/100], Loss: 0.0400\n",
      "Epoch [92/100], Loss: 0.0388\n",
      "Epoch [93/100], Loss: 0.0376\n",
      "Epoch [94/100], Loss: 0.0365\n",
      "Epoch [95/100], Loss: 0.0354\n",
      "Epoch [96/100], Loss: 0.0344\n",
      "Epoch [97/100], Loss: 0.0334\n",
      "Epoch [98/100], Loss: 0.0324\n",
      "Epoch [99/100], Loss: 0.0315\n",
      "Epoch [100/100], Loss: 0.0306\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "class CNNModel(nn.Module):  \n",
    "    \"\"\"\n",
    "    This class represents an AI model.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor for Model class.\n",
    "  \n",
    "        Parameters\n",
    "        ----------\n",
    "        self : object\n",
    "            The instance of the object passed by Python.\n",
    "        \"\"\"\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "                        nn.Conv2d(3, 32, (3,3)),\n",
    "                        nn.MaxPool2d((2, 2)),\n",
    "                        nn.LeakyReLU(0.1),\n",
    "                        nn.Conv2d(32, 64, (3,3)),\n",
    "                        nn.MaxPool2d((2, 2)),\n",
    "                        nn.LeakyReLU(0.1)\n",
    "                    )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "                        nn.Linear(64, 256),\n",
    "                        nn.LeakyReLU(0.1),\n",
    "                        nn.Linear(256, 128),\n",
    "                        nn.LeakyReLU(0.1),\n",
    "                        nn.Linear(128, 3)\n",
    "                    )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        #x = x.view(x.shape[0], 32, 4).mean(2) # GAP â€“ do not remove this line\n",
    "        x = nn.AvgPool2d(x.shape[2:])(x).view(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def preprocess(self, X):\n",
    "        # Normalize the data by replacing outliers with NaN\n",
    "        X[X > 255] = np.nan\n",
    "        X[X < 0] = np.nan\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5])\n",
    "            #transforms.RandomRotation(45)\n",
    "        ])\n",
    "\n",
    "        # Replace NaN values with the mean of the channel\n",
    "        for i in range(X.shape[0]):\n",
    "            image = X[i, :, :, :]\n",
    "\n",
    "            if np.isnan(image).any():\n",
    "                channel_means = np.nanmean(image.astype('float64'), axis=(1, 2))\n",
    "                if np.isnan(channel_means).any():\n",
    "                    print(\"There are NaN values in the channel means\")\n",
    "\n",
    "                image[0][np.isnan(image[0])] = channel_means[0]\n",
    "                image[1][np.isnan(image[1])] = channel_means[1]\n",
    "                image[2][np.isnan(image[2])] = channel_means[2]\n",
    "\n",
    "            img_transposed = np.transpose(image, (1, 2, 0))\n",
    "            img_transformed = transform(img_transposed)\n",
    "            test = np.transpose(img_transformed, (0, 1, 2))\n",
    "            X[i, :, :, :] = test\n",
    "\n",
    "        return X\n",
    "    \n",
    "    def scale(self, X):\n",
    "        # Reshape and scale the data\n",
    "        reshaped = X.reshape(X.shape[0], -1)\n",
    "        scaler = StandardScaler()\n",
    "        reshaped_scaled = scaler.fit_transform(reshaped)\n",
    "\n",
    "        return reshaped_scaled\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the model using the input data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, channel, height, width)\n",
    "            Training data.\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            Target values.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns an instance of the trained model.\n",
    "        \"\"\"\n",
    "        X = self.preprocess(X)\n",
    "        #X = self.scale(X)\n",
    "\n",
    "        # Apply PCA\n",
    "        #self.pca = PCA(n_components=0.99)\n",
    "        #self.pca.fit(X)\n",
    "        # Define the fc1 using the number of components from PCA\n",
    "        #self.fc1 = nn.Linear(self.pca.n_components_, self.n_hidden1)\n",
    "        #X_pca = self.pca.transform(X)\n",
    "\n",
    "        # Remove data with a corresponding NaN value in y\n",
    "        X = X[~np.isnan(y)]\n",
    "        y = y[~np.isnan(y)]\n",
    "\n",
    "        # Calculate class weights for imbalanced data\n",
    "        class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "        class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "        # Convert to tensors\n",
    "        X = torch.from_numpy(X).float()\n",
    "        y = torch.from_numpy(y).long()\n",
    "\n",
    "        # Initialize dataloader\n",
    "        train_loader = DataLoader(TensorDataset(X, y), shuffle=True, batch_size=512)\n",
    "\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        loss_fn = nn.CrossEntropyLoss(class_weights)\n",
    "\n",
    "        # Train the model\n",
    "        num_epochs = 200\n",
    "        #prev_loss = 99999\n",
    "        for i in range(num_epochs):\n",
    "            epoch_loss = 0\n",
    "            for _, data in enumerate(train_loader):\n",
    "                inputs, labels = data\n",
    "                outputs = self(inputs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                #if loss.item() > prev_loss:\n",
    "                #    break\n",
    "                #prev_loss = loss.item()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            epoch_loss = epoch_loss / len(train_loader)\n",
    "            print (\"Epoch: {}, Loss: {}\".format(i, epoch_loss))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained model to make predictions.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, channel, height, width)\n",
    "            Input data.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray of shape (n_samples,)\n",
    "        Predicted target values per element in X.\n",
    "           \n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        #X = self.preprocess(X)\n",
    "        #X = self.scale(X)\n",
    "        #X = self.pca.transform(X)\n",
    "        X = torch.from_numpy(X).float()\n",
    "        with torch.no_grad():\n",
    "            outputs = self(X)\n",
    "\n",
    "        return torch.argmax(outputs, dim=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel(nn.Module):  \n",
    "    \"\"\"\n",
    "    This class represents an AI model.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor for Model class.\n",
    "  \n",
    "        Parameters\n",
    "        ----------\n",
    "        self : object\n",
    "            The instance of the object passed by Python.\n",
    "        \"\"\"\n",
    "        super(MLPModel, self).__init__()\n",
    "        self.lrelu = nn.LeakyReLU(0.1)\n",
    "        self.n_hidden1 = 1024\n",
    "        self.n_hidden2 = 512\n",
    "        self.n_hidden3 = 256\n",
    "        self.n_classes = 3\n",
    "        self.fc2 = nn.Linear(self.n_hidden1, self.n_hidden2)\n",
    "        self.fc3 = nn.Linear(self.n_hidden2, self.n_hidden3)\n",
    "        self.fc4 = nn.Linear(self.n_hidden3, self.n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.lrelu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.lrelu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.lrelu(x)\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def preprocess(self, X):\n",
    "        # Normalize the data by replacing outliers with NaN\n",
    "        X[X > 255] = np.nan\n",
    "        X[X < 0] = np.nan\n",
    "\n",
    "        # Replace NaN values with the mean of the channel\n",
    "        for i in range(X.shape[0]):\n",
    "            image = X[i, :, :, :]\n",
    "\n",
    "            if np.isnan(image).any():\n",
    "                channel_means = np.nanmean(image.astype('float64'), axis=(1, 2))\n",
    "                if np.isnan(channel_means).any():\n",
    "                    print(\"There are NaN values in the channel means\")\n",
    "\n",
    "                image[0][np.isnan(image[0])] = channel_means[0]\n",
    "                image[1][np.isnan(image[1])] = channel_means[1]\n",
    "                image[2][np.isnan(image[2])] = channel_means[2]\n",
    "\n",
    "            X[i, :, :, :] = image\n",
    "\n",
    "        return X\n",
    "    \n",
    "    def scale(self, X):\n",
    "        # Reshape and scale the data\n",
    "        reshaped = X.reshape(X.shape[0], -1)\n",
    "        scaler = StandardScaler()\n",
    "        reshaped_scaled = scaler.fit_transform(reshaped)\n",
    "\n",
    "        return reshaped_scaled\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the model using the input data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, channel, height, width)\n",
    "            Training data.\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            Target values.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns an instance of the trained model.\n",
    "        \"\"\"\n",
    "        X = self.preprocess(X)\n",
    "        X = self.scale(X)\n",
    "\n",
    "        # Apply PCA\n",
    "        self.pca = PCA(n_components=0.99)\n",
    "        self.pca.fit(X)\n",
    "        # Define the fc1 using the number of components from PCA\n",
    "        self.fc1 = nn.Linear(self.pca.n_components_, self.n_hidden1)\n",
    "        X_pca = self.pca.transform(X)\n",
    "\n",
    "        # Remove data with a corresponding NaN value in y\n",
    "        X_pca = X_pca[~np.isnan(y)]\n",
    "        y = y[~np.isnan(y)]\n",
    "\n",
    "        # Calculate class weights for imbalanced data\n",
    "        class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "        class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "        # Convert to tensors\n",
    "        X = torch.from_numpy(X_pca).float()\n",
    "        y = torch.from_numpy(y).long()\n",
    "\n",
    "        # Initialize dataloader\n",
    "        train_loader = DataLoader(TensorDataset(X, y), shuffle=True, batch_size=64)\n",
    "\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        loss_fn = nn.CrossEntropyLoss(class_weights)\n",
    "\n",
    "        # Train the model\n",
    "        num_epochs = 200\n",
    "        for i in range(num_epochs):\n",
    "            epoch_loss = 0\n",
    "            for _, data in enumerate(train_loader):\n",
    "                inputs, labels = data\n",
    "                outputs = self(inputs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            epoch_loss = epoch_loss / len(train_loader)\n",
    "            print (\"Epoch: {}, Loss: {}\".format(i, epoch_loss))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained model to make predictions.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, channel, height, width)\n",
    "            Input data.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray of shape (n_samples,)\n",
    "        Predicted target values per element in X.\n",
    "           \n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        X = self.preprocess(X)\n",
    "        X = self.scale(X)\n",
    "        X = self.pca.transform(X)\n",
    "        X = torch.from_numpy(X).float()\n",
    "        with torch.no_grad():\n",
    "            outputs = self(X)\n",
    "\n",
    "        return torch.argmax(outputs, dim=1).numpy()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
