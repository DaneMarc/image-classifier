{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d017333",
   "metadata": {},
   "source": [
    "# Final Assessment: Making Prediction on a Dataset without Domain Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9045d90d",
   "metadata": {},
   "source": [
    "## Tasks & Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class CNNModel(nn.Module):  \n",
    "    \"\"\"\n",
    "    This class represents an AI model.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor for Model class.\n",
    "  \n",
    "        Parameters\n",
    "        ----------\n",
    "        self : object\n",
    "            The instance of the object passed by Python.\n",
    "        \"\"\"\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "                        nn.Conv2d(3, 32, (3,3)),\n",
    "                        nn.MaxPool2d((2, 2)),\n",
    "                        nn.LeakyReLU(0.1),\n",
    "                        nn.Conv2d(32, 64, (3,3)),\n",
    "                        nn.MaxPool2d((2, 2)),\n",
    "                        nn.LeakyReLU(0.1)\n",
    "                    )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "                        nn.Linear(64, 256),\n",
    "                        nn.LeakyReLU(0.1),\n",
    "                        nn.Linear(256, 128),\n",
    "                        nn.LeakyReLU(0.1),\n",
    "                        nn.Linear(128, 3)\n",
    "                    )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        #x = x.view(x.shape[0], 32, 4).mean(2) # GAP â€“ do not remove this line\n",
    "        x = nn.AvgPool2d(x.shape[2:])(x).view(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def preprocess(self, X):\n",
    "        # Normalize the data by replacing outliers with NaN\n",
    "        X[X > 255] = np.nan\n",
    "        X[X < 0] = np.nan\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5])\n",
    "            #transforms.RandomRotation(45)\n",
    "        ])\n",
    "\n",
    "        # Replace NaN values with the mean of the channel\n",
    "        for i in range(X.shape[0]):\n",
    "            image = X[i, :, :, :]\n",
    "\n",
    "            if np.isnan(image).any():\n",
    "                channel_means = np.nanmean(image.astype('float64'), axis=(1, 2))\n",
    "                if np.isnan(channel_means).any():\n",
    "                    print(\"There are NaN values in the channel means\")\n",
    "\n",
    "                image[0][np.isnan(image[0])] = channel_means[0]\n",
    "                image[1][np.isnan(image[1])] = channel_means[1]\n",
    "                image[2][np.isnan(image[2])] = channel_means[2]\n",
    "\n",
    "            img_transposed = np.transpose(image, (1, 2, 0))\n",
    "            img_transformed = transform(img_transposed)\n",
    "            test = np.transpose(img_transformed, (0, 1, 2))\n",
    "            X[i, :, :, :] = test\n",
    "\n",
    "        return X\n",
    "    \n",
    "    def scale(self, X):\n",
    "        # Reshape and scale the data\n",
    "        reshaped = X.reshape(X.shape[0], -1)\n",
    "        scaler = StandardScaler()\n",
    "        reshaped_scaled = scaler.fit_transform(reshaped)\n",
    "\n",
    "        return reshaped_scaled\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the model using the input data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, channel, height, width)\n",
    "            Training data.\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            Target values.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns an instance of the trained model.\n",
    "        \"\"\"\n",
    "        X = self.preprocess(X)\n",
    "        #X = self.scale(X)\n",
    "\n",
    "        # Apply PCA\n",
    "        #self.pca = PCA(n_components=0.99)\n",
    "        #self.pca.fit(X)\n",
    "        # Define the fc1 using the number of components from PCA\n",
    "        #self.fc1 = nn.Linear(self.pca.n_components_, self.n_hidden1)\n",
    "        #X_pca = self.pca.transform(X)\n",
    "\n",
    "        # Remove data with a corresponding NaN value in y\n",
    "        X = X[~np.isnan(y)]\n",
    "        y = y[~np.isnan(y)]\n",
    "\n",
    "        # Calculate class weights for imbalanced data\n",
    "        class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "        class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "        # Convert to tensors\n",
    "        X = torch.from_numpy(X).float()\n",
    "        y = torch.from_numpy(y).long()\n",
    "\n",
    "        # Initialize dataloader\n",
    "        train_loader = DataLoader(TensorDataset(X, y), shuffle=True, batch_size=512)\n",
    "\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        loss_fn = nn.CrossEntropyLoss(class_weights)\n",
    "\n",
    "        # Train the model\n",
    "        num_epochs = 200\n",
    "        #prev_loss = 99999\n",
    "        for i in range(num_epochs):\n",
    "            epoch_loss = 0\n",
    "            for _, data in enumerate(train_loader):\n",
    "                inputs, labels = data\n",
    "                outputs = self(inputs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                #if loss.item() > prev_loss:\n",
    "                #    break\n",
    "                #prev_loss = loss.item()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            epoch_loss = epoch_loss / len(train_loader)\n",
    "            print (\"Epoch: {}, Loss: {}\".format(i, epoch_loss))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained model to make predictions.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, channel, height, width)\n",
    "            Input data.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray of shape (n_samples,)\n",
    "        Predicted target values per element in X.\n",
    "           \n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        #X = self.preprocess(X)\n",
    "        #X = self.scale(X)\n",
    "        #X = self.pca.transform(X)\n",
    "        X = torch.from_numpy(X).float()\n",
    "        with torch.no_grad():\n",
    "            outputs = self(X)\n",
    "\n",
    "        return torch.argmax(outputs, dim=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "a44b7aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class MLPModel(nn.Module):  \n",
    "    \"\"\"\n",
    "    This class represents an AI model.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor for Model class.\n",
    "  \n",
    "        Parameters\n",
    "        ----------\n",
    "        self : object\n",
    "            The instance of the object passed by Python.\n",
    "        \"\"\"\n",
    "        super(MLPModel, self).__init__()\n",
    "        self.lrelu = nn.LeakyReLU(0.1)\n",
    "        self.n_hidden1 = 1024\n",
    "        self.n_hidden2 = 512\n",
    "        self.n_hidden3 = 256\n",
    "        self.n_classes = 3\n",
    "        self.fc2 = nn.Linear(self.n_hidden1, self.n_hidden2)\n",
    "        self.fc3 = nn.Linear(self.n_hidden2, self.n_hidden3)\n",
    "        self.fc4 = nn.Linear(self.n_hidden3, self.n_classes)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.lrelu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.lrelu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.lrelu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.log_softmax(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def preprocess(self, X):\n",
    "        # Normalize the data by replacing outliers with NaN\n",
    "        X[X > 255] = np.nan\n",
    "        X[X < 0] = np.nan\n",
    "\n",
    "        # Replace NaN values with the mean of the channel\n",
    "        for i in range(X.shape[0]):\n",
    "            image = X[i, :, :, :]\n",
    "\n",
    "            if np.isnan(image).any():\n",
    "                channel_means = np.nanmean(image.astype('float64'), axis=(1, 2))\n",
    "                if np.isnan(channel_means).any():\n",
    "                    print(\"There are NaN values in the channel means\")\n",
    "\n",
    "                image[0][np.isnan(image[0])] = channel_means[0]\n",
    "                image[1][np.isnan(image[1])] = channel_means[1]\n",
    "                image[2][np.isnan(image[2])] = channel_means[2]\n",
    "\n",
    "            X[i, :, :, :] = image\n",
    "\n",
    "        return X\n",
    "    \n",
    "    def scale(self, X):\n",
    "        # Reshape and scale the data\n",
    "        reshaped = X.reshape(X.shape[0], -1)\n",
    "        scaler = StandardScaler()\n",
    "        reshaped_scaled = scaler.fit_transform(reshaped)\n",
    "\n",
    "        return reshaped_scaled\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the model using the input data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, channel, height, width)\n",
    "            Training data.\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            Target values.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns an instance of the trained model.\n",
    "        \"\"\"\n",
    "        X = self.preprocess(X)\n",
    "        X = self.scale(X)\n",
    "\n",
    "        # Apply PCA\n",
    "        self.pca = PCA(n_components=0.99)\n",
    "        self.pca.fit(X)\n",
    "        # Define the fc1 using the number of components from PCA\n",
    "        self.fc1 = nn.Linear(self.pca.n_components_, self.n_hidden1)\n",
    "        X_pca = self.pca.transform(X)\n",
    "\n",
    "        # Remove data with a corresponding NaN value in y\n",
    "        X_pca = X_pca[~np.isnan(y)]\n",
    "        y = y[~np.isnan(y)]\n",
    "\n",
    "        # Calculate class weights for imbalanced data\n",
    "        class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "        class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "        # Convert to tensors\n",
    "        X = torch.from_numpy(X_pca).float()\n",
    "        y = torch.from_numpy(y).long()\n",
    "\n",
    "        # Initialize dataloader\n",
    "        train_loader = DataLoader(TensorDataset(X, y), shuffle=True, batch_size=64)\n",
    "\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        loss_fn = nn.CrossEntropyLoss(class_weights)\n",
    "\n",
    "        # Train the model\n",
    "        num_epochs = 100\n",
    "        for i in range(num_epochs):\n",
    "            epoch_loss = 0\n",
    "            for _, data in enumerate(train_loader):\n",
    "                inputs, labels = data\n",
    "                outputs = self(inputs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            epoch_loss = epoch_loss / len(train_loader)\n",
    "            print (\"Epoch: {}, Loss: {}\".format(i, epoch_loss))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained model to make predictions.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, channel, height, width)\n",
    "            Input data.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        ndarray of shape (n_samples,)\n",
    "        Predicted target values per element in X.\n",
    "           \n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        X = self.preprocess(X)\n",
    "        X = self.scale(X)\n",
    "        X = self.pca.transform(X)\n",
    "        X = torch.from_numpy(X).float()\n",
    "        with torch.no_grad():\n",
    "            outputs = self(X)\n",
    "\n",
    "        return torch.argmax(outputs, dim=1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02178d7",
   "metadata": {},
   "source": [
    "#### Local Evaluation\n",
    "\n",
    "You may test your solution locally by running the following code. Do note that the results may not reflect your performance in Coursemology. You should not be submitting the code below in Coursemology. The code here is meant only for you to do local testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4f4dd489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3064e0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "with open('data.npy', 'rb') as f:\n",
    "    data = np.load(f, allow_pickle=True).item()\n",
    "    X = data['image']\n",
    "    y = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "27c9fd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.879564479798884\n",
      "Epoch: 1, Loss: 0.321165699813817\n",
      "Epoch: 2, Loss: 0.11935280822217464\n",
      "Epoch: 3, Loss: 0.035500711547462524\n",
      "Epoch: 4, Loss: 0.016074027913320507\n",
      "Epoch: 5, Loss: 0.010740979283975714\n",
      "Epoch: 6, Loss: 0.003127313926155251\n",
      "Epoch: 7, Loss: 0.0013565582548098235\n",
      "Epoch: 8, Loss: 0.00044996448952981546\n",
      "Epoch: 9, Loss: 0.0002582809415675788\n",
      "Epoch: 10, Loss: 0.00016979354679918019\n",
      "Epoch: 11, Loss: 0.00013469944747772093\n",
      "Epoch: 12, Loss: 0.00010899466139341491\n",
      "Epoch: 13, Loss: 9.100555512673423e-05\n",
      "Epoch: 14, Loss: 7.308857798430049e-05\n",
      "Epoch: 15, Loss: 6.194561432789055e-05\n",
      "Epoch: 16, Loss: 5.538049874696692e-05\n",
      "Epoch: 17, Loss: 4.664921721701536e-05\n",
      "Epoch: 18, Loss: 3.795214633027098e-05\n",
      "Epoch: 19, Loss: 3.730957808550778e-05\n",
      "Epoch: 20, Loss: 3.42695875588106e-05\n",
      "Epoch: 21, Loss: 2.9910915981344186e-05\n",
      "Epoch: 22, Loss: 2.7275329213463574e-05\n",
      "Epoch: 23, Loss: 2.3614136525056154e-05\n",
      "Epoch: 24, Loss: 2.2741821966691703e-05\n",
      "Epoch: 25, Loss: 2.0058607628311006e-05\n",
      "Epoch: 26, Loss: 1.917381042108605e-05\n",
      "Epoch: 27, Loss: 1.7406452226734127e-05\n",
      "Epoch: 28, Loss: 1.5503276331032425e-05\n",
      "Epoch: 29, Loss: 1.5128898359909874e-05\n",
      "Epoch: 30, Loss: 1.3403799388273714e-05\n",
      "Epoch: 31, Loss: 1.3170300143109352e-05\n",
      "Epoch: 32, Loss: 1.1643255705879514e-05\n",
      "Epoch: 33, Loss: 1.1411822070668504e-05\n",
      "Epoch: 34, Loss: 1.0261553413599629e-05\n",
      "Epoch: 35, Loss: 1.0178745807590187e-05\n",
      "Epoch: 36, Loss: 9.498474128122325e-06\n",
      "Epoch: 37, Loss: 8.316217018373945e-06\n",
      "Epoch: 38, Loss: 8.566179752272768e-06\n",
      "Epoch: 39, Loss: 8.103115969542824e-06\n",
      "Epoch: 40, Loss: 7.54571154888057e-06\n",
      "Epoch: 41, Loss: 6.778399733161063e-06\n",
      "Epoch: 42, Loss: 6.867823028212139e-06\n",
      "Epoch: 43, Loss: 6.459440314572291e-06\n",
      "Epoch: 44, Loss: 6.183116947549688e-06\n",
      "Epoch: 45, Loss: 6.0629372087164115e-06\n",
      "Epoch: 46, Loss: 5.37033143528616e-06\n",
      "Epoch: 47, Loss: 5.2428277691384165e-06\n",
      "Epoch: 48, Loss: 5.008006518117917e-06\n",
      "Epoch: 49, Loss: 4.924531542717493e-06\n",
      "Epoch: 50, Loss: 4.582029658662135e-06\n",
      "Epoch: 51, Loss: 4.6264113900572225e-06\n",
      "Epoch: 52, Loss: 4.258492277751823e-06\n",
      "Epoch: 53, Loss: 4.121175385768544e-06\n",
      "Epoch: 54, Loss: 4.042023890080502e-06\n",
      "Epoch: 55, Loss: 3.9029547482445e-06\n",
      "Epoch: 56, Loss: 3.5992749727680546e-06\n",
      "Epoch: 57, Loss: 3.5494195965322825e-06\n",
      "Epoch: 58, Loss: 3.4957511560777846e-06\n",
      "Epoch: 59, Loss: 3.344299955188035e-06\n",
      "Epoch: 60, Loss: 3.084653081480831e-06\n",
      "Epoch: 61, Loss: 3.0723039178672088e-06\n",
      "Epoch: 62, Loss: 2.9322430089303304e-06\n",
      "Epoch: 63, Loss: 2.929982037364204e-06\n",
      "Epoch: 64, Loss: 2.7167118010102306e-06\n",
      "Epoch: 65, Loss: 2.6427201276333477e-06\n",
      "Epoch: 66, Loss: 2.5434483085199325e-06\n",
      "Epoch: 67, Loss: 2.396626580075217e-06\n",
      "Epoch: 68, Loss: 2.462690491557697e-06\n",
      "Epoch: 69, Loss: 2.2787125139124377e-06\n",
      "Epoch: 70, Loss: 2.1974733630105292e-06\n",
      "Epoch: 71, Loss: 2.0819659020211524e-06\n",
      "Epoch: 72, Loss: 2.0119980982181496e-06\n",
      "Epoch: 73, Loss: 1.932768877604512e-06\n",
      "Epoch: 74, Loss: 1.8597853936994767e-06\n",
      "Epoch: 75, Loss: 1.8170613900012441e-06\n",
      "Epoch: 76, Loss: 1.8761767281633897e-06\n",
      "Epoch: 77, Loss: 1.770712720378248e-06\n",
      "Epoch: 78, Loss: 1.694142803124713e-06\n",
      "Epoch: 79, Loss: 1.6743602778611553e-06\n",
      "Epoch: 80, Loss: 1.6078823949629697e-06\n",
      "Epoch: 81, Loss: 1.5223454445153824e-06\n",
      "Epoch: 82, Loss: 1.4565292850740072e-06\n",
      "Epoch: 83, Loss: 1.4695791290902176e-06\n",
      "Epoch: 84, Loss: 1.3951845065438082e-06\n",
      "Epoch: 85, Loss: 1.4137925284779378e-06\n",
      "Epoch: 86, Loss: 1.2972423878882428e-06\n",
      "Epoch: 87, Loss: 1.2792388980181606e-06\n",
      "Epoch: 88, Loss: 1.2446228091399412e-06\n",
      "Epoch: 89, Loss: 1.2104466280235912e-06\n",
      "Epoch: 90, Loss: 1.2221011351007168e-06\n",
      "Epoch: 91, Loss: 1.1477211263059576e-06\n",
      "Epoch: 92, Loss: 1.1081052811910873e-06\n",
      "Epoch: 93, Loss: 1.059921961034495e-06\n",
      "Epoch: 94, Loss: 1.0751119593867376e-06\n",
      "Epoch: 95, Loss: 1.0284717122148493e-06\n",
      "Epoch: 96, Loss: 9.792161661706003e-07\n",
      "Epoch: 97, Loss: 9.333043397578582e-07\n",
      "Epoch: 98, Loss: 9.480272668755591e-07\n",
      "Epoch: 99, Loss: 9.311109150727331e-07\n",
      "F1 Score (macro): 0.61\n"
     ]
    }
   ],
   "source": [
    "# Split train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "# Filter test data that contains no labels\n",
    "# In Coursemology, the test data is guaranteed to have labels\n",
    "nan_indices = np.argwhere(np.isnan(y_test)).squeeze()\n",
    "mask = np.ones(y_test.shape, bool)\n",
    "mask[nan_indices] = False\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]\n",
    "\n",
    "# Train and predict\n",
    "model = MLPModel()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model predition\n",
    "# Learn more: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "print(\"F1 Score (macro): {0:.2f}\".format(f1_score(y_test, y_pred, average='macro'))) # You may encounter errors, you are expected to figure out what's the issue."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
